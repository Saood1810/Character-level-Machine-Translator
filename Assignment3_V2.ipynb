{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Package Imports"
      ],
      "metadata": {
        "id": "ZLj1MNdHgVRg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Muhammad Sa'ood Shah 221021204\n",
        "#Yruvan Ramjan 221004686\n",
        "import numpy as np\n",
        "import keras\n",
        "import os\n",
        "from pathlib import Path\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from keras.layers import Bidirectional, LSTM,Embedding,GRU,Dense,Concatenate\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "0iWluG7qObTJ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "-VEcJlgOg9jh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NFHfKypcIVNM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d72d78f8-2128-4122-9046-1f36fa705a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "data_path='/content/drive/MyDrive/Colab Notebooks/fra.txt'\n",
        "\n",
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre Processing"
      ],
      "metadata": {
        "id": "IhB3-lWGgSrZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "di9uFLKIIVBP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edbc0b72-e358-4a4a-fb6b-65eed7c6fdc1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input 1: Go.\n",
            "Target 1: \tVa !\n",
            "\n",
            "--------------------------------------------------\n",
            "Input 2: Go.\n",
            "Target 2: \tMarche.\n",
            "\n",
            "--------------------------------------------------\n",
            "Input 3: Go.\n",
            "Target 3: \tEn route !\n",
            "\n",
            "--------------------------------------------------\n",
            "Input 4: Go.\n",
            "Target 4: \tBouge !\n",
            "\n",
            "--------------------------------------------------\n",
            "Input 5: Hi.\n",
            "Target 5: \tSalut !\n",
            "\n",
            "--------------------------------------------------\n",
            "Input 6: Hi.\n",
            "Target 6: \tSalut.\n",
            "\n",
            "--------------------------------------------------\n",
            "Input 7: Run!\n",
            "Target 7: \tCours !\n",
            "\n",
            "--------------------------------------------------\n",
            "Input 8: Run!\n",
            "Target 8: \tCourez !\n",
            "\n",
            "--------------------------------------------------\n",
            "Input 9: Run!\n",
            "Target 9: \tPrenez vos jambes à vos cous !\n",
            "\n",
            "--------------------------------------------------\n",
            "Input 10: Run!\n",
            "Target 10: \tFile !\n",
            "\n",
            "--------------------------------------------------\n",
            "Go.\n",
            "Number of samples: 10000\n",
            "Number of unique input tokens: 70\n",
            "Number of unique output tokens: 91\n",
            "Max sequence length for inputs: 14\n",
            "Max sequence length for outputs: 59\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Vectorize the data.\n",
        "input_texts = []\n",
        "target_texts = []\n",
        "input_characters = set()\n",
        "target_characters = set()\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "    input_text, target_text, _ = line.split(\"\\t\")\n",
        "    # We use \"tab\" as the \"start sequence\" character\n",
        "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
        "    target_text = \"\\t\" + target_text + \"\\n\"\n",
        "    input_texts.append(input_text)\n",
        "    target_texts.append(target_text)\n",
        "    for char in input_text:\n",
        "        if char not in input_characters:\n",
        "            input_characters.add(char)\n",
        "    for char in target_text:\n",
        "        if char not in target_characters:\n",
        "\n",
        "            target_characters.add(char)\n",
        "for i in range(10):\n",
        "    print(f\"Input {i+1}: {input_texts[i]}\")\n",
        "    print(f\"Target {i+1}: {target_texts[i]}\")\n",
        "    print(\"-\" * 50)\n",
        "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    lines = f.read().split(\"\\n\")\n",
        "print(input_texts[0])\n",
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
        "\n",
        "print(\"Number of samples:\", len(input_texts))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
        "input_train, input_test, target_train, target_test = train_test_split(input_texts, target_texts, test_size=0.2, random_state=42)\n",
        "encoder_input_data = np.zeros(\n",
        "    (len(input_train), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_train), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_train), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype=\"float32\",\n",
        ")\n",
        "\n",
        "for i, (input_train, target_train) in enumerate(zip(input_train, target_train)):\n",
        "    for t, char in enumerate(input_train):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
        "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
        "    for t, char in enumerate(target_train):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
        "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
        "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_texts(texts, token_index, max_seq_length, num_tokens):\n",
        "    data = np.zeros((len(texts), max_seq_length, num_tokens), dtype=\"float32\")\n",
        "    for i, text in enumerate(texts):\n",
        "        for t, char in enumerate(text):\n",
        "            data[i, t, token_index[char]] = 1.0\n",
        "        # Padding with space character\n",
        "        data[i, t + 1:, token_index[\" \"]] = 1.0\n",
        "    return data\n",
        "\n",
        "# Vectorize the test data\n",
        "encoder_input_data_test = vectorize_texts(input_test, input_token_index, max_encoder_seq_length, num_encoder_tokens)\n",
        "decoder_input_data_test = vectorize_texts(target_test, target_token_index, max_decoder_seq_length, num_decoder_tokens)\n"
      ],
      "metadata": {
        "id": "LaP5ehk7BIEV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-decoder model with bidirectional LSTM encoder and an LSTM decoder"
      ],
      "metadata": {
        "id": "Z9Dt4QGBy2IX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R1kAb1tcwTQn"
      },
      "outputs": [],
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = Bidirectional(LSTM(latent_dim, return_state=True))\n",
        "encoder_outputs,forward_h,forward_c ,backward_h, backward_c = encoder(encoder_inputs)\n",
        "\n",
        "state_h=Concatenate()([forward_h,backward_h])\n",
        "state_c=Concatenate()([forward_c,backward_c])\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h, state_c] #final state of encoder\n",
        "# Captures semantic info of input seq and represents it as a context Vector\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_latent_dim = latent_dim * 2\n",
        "decoder_lstm = LSTM(decoder_latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)#Output of previous of decoder at prev timetstamp and encoder states as input\n",
        "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YGyI382wTQp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cefc39d-1199-40bd-d673-c2f11eb09d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - 11s 67ms/step - loss: 1.1681 - accuracy: 0.7263 - val_loss: 0.8905 - val_accuracy: 0.7447\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 5s 53ms/step - loss: 1.0813 - accuracy: 0.7478 - val_loss: 0.9084 - val_accuracy: 0.7545\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.8170 - accuracy: 0.7768 - val_loss: 0.7562 - val_accuracy: 0.7911\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 6s 57ms/step - loss: 0.7187 - accuracy: 0.8002 - val_loss: 0.6715 - val_accuracy: 0.8100\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 6s 56ms/step - loss: 0.6472 - accuracy: 0.8140 - val_loss: 0.6193 - val_accuracy: 0.8217\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 6s 55ms/step - loss: 0.6011 - accuracy: 0.8258 - val_loss: 0.5818 - val_accuracy: 0.8311\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.5678 - accuracy: 0.8347 - val_loss: 0.5530 - val_accuracy: 0.8391\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.5410 - accuracy: 0.8424 - val_loss: 0.5281 - val_accuracy: 0.8460\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 5s 51ms/step - loss: 0.5203 - accuracy: 0.8481 - val_loss: 0.5094 - val_accuracy: 0.8512\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 5s 50ms/step - loss: 0.5020 - accuracy: 0.8532 - val_loss: 0.4971 - val_accuracy: 0.8538\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 5s 51ms/step - loss: 0.4849 - accuracy: 0.8570 - val_loss: 0.4832 - val_accuracy: 0.8571\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 5s 50ms/step - loss: 0.4703 - accuracy: 0.8612 - val_loss: 0.4736 - val_accuracy: 0.8596\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 5s 52ms/step - loss: 0.4567 - accuracy: 0.8644 - val_loss: 0.4580 - val_accuracy: 0.8632\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 5s 53ms/step - loss: 0.4429 - accuracy: 0.8681 - val_loss: 0.4474 - val_accuracy: 0.8660\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 5s 52ms/step - loss: 0.4310 - accuracy: 0.8713 - val_loss: 0.4353 - val_accuracy: 0.8700\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 5s 51ms/step - loss: 0.4186 - accuracy: 0.8743 - val_loss: 0.4259 - val_accuracy: 0.8725\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 5s 52ms/step - loss: 0.4059 - accuracy: 0.8782 - val_loss: 0.4161 - val_accuracy: 0.8752\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 5s 51ms/step - loss: 0.3937 - accuracy: 0.8817 - val_loss: 0.4095 - val_accuracy: 0.8783\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.3808 - accuracy: 0.8853 - val_loss: 0.3961 - val_accuracy: 0.8819\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.3684 - accuracy: 0.8890 - val_loss: 0.3938 - val_accuracy: 0.8829\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - 5s 52ms/step - loss: 0.3570 - accuracy: 0.8924 - val_loss: 0.3831 - val_accuracy: 0.8861\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.3457 - accuracy: 0.8953 - val_loss: 0.3764 - val_accuracy: 0.8874\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.3326 - accuracy: 0.8995 - val_loss: 0.3684 - val_accuracy: 0.8908\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - 5s 52ms/step - loss: 0.3197 - accuracy: 0.9036 - val_loss: 0.3601 - val_accuracy: 0.8942\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 5s 51ms/step - loss: 0.3065 - accuracy: 0.9075 - val_loss: 0.3554 - val_accuracy: 0.8945\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - 5s 53ms/step - loss: 0.2931 - accuracy: 0.9115 - val_loss: 0.3508 - val_accuracy: 0.8967\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - 5s 51ms/step - loss: 0.2794 - accuracy: 0.9156 - val_loss: 0.3448 - val_accuracy: 0.8990\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - 5s 51ms/step - loss: 0.2660 - accuracy: 0.9196 - val_loss: 0.3401 - val_accuracy: 0.9001\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - 5s 53ms/step - loss: 0.2524 - accuracy: 0.9235 - val_loss: 0.3351 - val_accuracy: 0.9018\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - 5s 54ms/step - loss: 0.2382 - accuracy: 0.9282 - val_loss: 0.3304 - val_accuracy: 0.9039\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - 5s 51ms/step - loss: 0.2239 - accuracy: 0.9322 - val_loss: 0.3282 - val_accuracy: 0.9053\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - 5s 52ms/step - loss: 0.2092 - accuracy: 0.9369 - val_loss: 0.3279 - val_accuracy: 0.9057\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - 5s 51ms/step - loss: 0.1938 - accuracy: 0.9415 - val_loss: 0.3259 - val_accuracy: 0.9074\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - 5s 53ms/step - loss: 0.4764 - accuracy: 0.8760 - val_loss: 0.3956 - val_accuracy: 0.8818\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - 5s 52ms/step - loss: 0.3123 - accuracy: 0.9049 - val_loss: 0.3465 - val_accuracy: 0.8980\n",
            "Epoch 35: early stopping\n"
          ]
        }
      ],
      "source": [
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "model.compile(\n",
        "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=[es],\n",
        "    validation_split=0.2,\n",
        ")\n",
        "# Save model\n",
        "model.save(\"/content/drive/MyDrive/Colab Notebooks/BiDirectional_lstm_Encoder_lstm_decoderV3.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder-decoder model with bidirectional GRU encoder and a GRU decoder"
      ],
      "metadata": {
        "id": "dwev3B9o6TrP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwLpBoXl6WSH"
      },
      "outputs": [],
      "source": [
        "# Define an input sequence and process it.\n",
        "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
        "encoder = Bidirectional(GRU(latent_dim, return_state=True))\n",
        "encoder_outputs,forward_h,backward_h = encoder(encoder_inputs)\n",
        "\n",
        "state_h=Concatenate()([forward_h,backward_h])\n",
        "\n",
        "# We discard `encoder_outputs` and only keep the states.\n",
        "encoder_states = [state_h]\n",
        "\n",
        "# Set up the decoder, using `encoder_states` as initial state.\n",
        "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
        "\n",
        "# We set up our decoder to return full output sequences,\n",
        "# and to return internal states as well. We don't use the\n",
        "# return states in the training model, but we will use them in inference.\n",
        "decoder_latent_dim = latent_dim * 2\n",
        "decoder_gru = GRU(decoder_latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=state_h)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn\n",
        "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
        "model2 = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "model2.compile(\n",
        "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model2.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=[es],\n",
        "    validation_split=0.2,\n",
        ")\n",
        "# Save model\n",
        "model2.save(\"/content/drive/MyDrive/Colab Notebooks/BiDirectional_gru_Encoder_gru_decoderV2.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4miPguMAfrBc",
        "outputId": "52a256fd-4cbb-4753-aba5-a9fdb1c14a8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100/100 [==============================] - 8s 36ms/step - loss: 1.2896 - accuracy: 0.7305 - val_loss: 0.8603 - val_accuracy: 0.7593\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 2s 20ms/step - loss: 0.7505 - accuracy: 0.7901 - val_loss: 0.6530 - val_accuracy: 0.8119\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.6073 - accuracy: 0.8254 - val_loss: 0.5727 - val_accuracy: 0.8324\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.5464 - accuracy: 0.8416 - val_loss: 0.5268 - val_accuracy: 0.8448\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.5106 - accuracy: 0.8505 - val_loss: 0.4977 - val_accuracy: 0.8543\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 0.4846 - accuracy: 0.8579 - val_loss: 0.4783 - val_accuracy: 0.8595\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 0.4636 - accuracy: 0.8634 - val_loss: 0.4593 - val_accuracy: 0.8651\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.4425 - accuracy: 0.8691 - val_loss: 0.4411 - val_accuracy: 0.8693\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.4245 - accuracy: 0.8740 - val_loss: 0.4279 - val_accuracy: 0.8732\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.4054 - accuracy: 0.8788 - val_loss: 0.4127 - val_accuracy: 0.8775\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.3881 - accuracy: 0.8838 - val_loss: 0.3985 - val_accuracy: 0.8819\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.3699 - accuracy: 0.8887 - val_loss: 0.3875 - val_accuracy: 0.8851\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.3533 - accuracy: 0.8937 - val_loss: 0.3763 - val_accuracy: 0.8891\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.3368 - accuracy: 0.8985 - val_loss: 0.3658 - val_accuracy: 0.8916\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 2s 21ms/step - loss: 0.3184 - accuracy: 0.9044 - val_loss: 0.3561 - val_accuracy: 0.8944\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.3008 - accuracy: 0.9092 - val_loss: 0.3454 - val_accuracy: 0.8981\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 2s 20ms/step - loss: 0.2838 - accuracy: 0.9142 - val_loss: 0.3408 - val_accuracy: 0.9004\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 0.2675 - accuracy: 0.9191 - val_loss: 0.3319 - val_accuracy: 0.9036\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.2501 - accuracy: 0.9243 - val_loss: 0.3267 - val_accuracy: 0.9055\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 0.2341 - accuracy: 0.9293 - val_loss: 0.3232 - val_accuracy: 0.9076\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 0.2176 - accuracy: 0.9339 - val_loss: 0.3196 - val_accuracy: 0.9084\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 0.2019 - accuracy: 0.9390 - val_loss: 0.3190 - val_accuracy: 0.9101\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 2s 22ms/step - loss: 0.1868 - accuracy: 0.9433 - val_loss: 0.3167 - val_accuracy: 0.9118\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 0.1719 - accuracy: 0.9479 - val_loss: 0.3209 - val_accuracy: 0.9115\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 2s 19ms/step - loss: 0.1581 - accuracy: 0.9520 - val_loss: 0.3201 - val_accuracy: 0.9126\n",
            "Epoch 25: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double Layer GRU Encoder and GRU Decoder"
      ],
      "metadata": {
        "id": "OnV9YvGyhDXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latent_dim = 256  # Latent dimensionality of the encoding space\n",
        "\n",
        "# Define input sequence shape\n",
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder_first_layer = GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "encoder_outputs_first_layer, state_h1 = encoder_first_layer(encoder_inputs)\n",
        "\n",
        "# Adding the second GRU layer\n",
        "encoder_second_layer = GRU(latent_dim, return_state=True)\n",
        "encoder_outputs_second_layer, state_h2 = encoder_second_layer(encoder_outputs_first_layer)\n",
        "\n",
        "# Define the decoder with the same latent dimension\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_gru = GRU(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _ = decoder_gru(decoder_inputs, initial_state=[state_h2])\n",
        "\n",
        "# Dense layer to generate output tokens\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Define the model that will turn encoder_input_data & decoder_input_data into decoder_target_data\n",
        "\n",
        "model3 = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "# Compile the model\n",
        "model3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "vxPCEgTa7YYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2)\n",
        "model3.compile(\n",
        "    optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "model3.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=[es],\n",
        "    validation_split=0.2,\n",
        ")\n",
        "# Save model\n",
        "model3.save(\"/content/drive/MyDrive/Colab Notebooks/doubleLayer_gru_Encoder_gru_decoderV2.keras\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu-N_hk09Fn_",
        "outputId": "0cefd7d4-5345-4772-e6a2-35fbabfe2c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "100/100 [==============================] - 7s 29ms/step - loss: 1.4295 - accuracy: 0.7182 - val_loss: 0.9362 - val_accuracy: 0.7393\n",
            "Epoch 2/100\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.8332 - accuracy: 0.7709 - val_loss: 0.7397 - val_accuracy: 0.7953\n",
            "Epoch 3/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.6882 - accuracy: 0.8057 - val_loss: 0.6403 - val_accuracy: 0.8126\n",
            "Epoch 4/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.6178 - accuracy: 0.8206 - val_loss: 0.5918 - val_accuracy: 0.8272\n",
            "Epoch 5/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.5800 - accuracy: 0.8298 - val_loss: 0.5640 - val_accuracy: 0.8343\n",
            "Epoch 6/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.5518 - accuracy: 0.8373 - val_loss: 0.5419 - val_accuracy: 0.8393\n",
            "Epoch 7/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.5310 - accuracy: 0.8427 - val_loss: 0.5213 - val_accuracy: 0.8458\n",
            "Epoch 8/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.5133 - accuracy: 0.8478 - val_loss: 0.5056 - val_accuracy: 0.8490\n",
            "Epoch 9/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.4973 - accuracy: 0.8522 - val_loss: 0.4921 - val_accuracy: 0.8533\n",
            "Epoch 10/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.4820 - accuracy: 0.8561 - val_loss: 0.4783 - val_accuracy: 0.8580\n",
            "Epoch 11/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.4670 - accuracy: 0.8608 - val_loss: 0.4626 - val_accuracy: 0.8634\n",
            "Epoch 12/100\n",
            "100/100 [==============================] - 2s 18ms/step - loss: 0.4514 - accuracy: 0.8667 - val_loss: 0.4522 - val_accuracy: 0.8665\n",
            "Epoch 13/100\n",
            "100/100 [==============================] - 2s 16ms/step - loss: 0.4385 - accuracy: 0.8699 - val_loss: 0.4414 - val_accuracy: 0.8694\n",
            "Epoch 14/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.4248 - accuracy: 0.8738 - val_loss: 0.4287 - val_accuracy: 0.8729\n",
            "Epoch 15/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.4133 - accuracy: 0.8769 - val_loss: 0.4189 - val_accuracy: 0.8762\n",
            "Epoch 16/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.4033 - accuracy: 0.8791 - val_loss: 0.4119 - val_accuracy: 0.8780\n",
            "Epoch 17/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3923 - accuracy: 0.8822 - val_loss: 0.4018 - val_accuracy: 0.8802\n",
            "Epoch 18/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3822 - accuracy: 0.8851 - val_loss: 0.3951 - val_accuracy: 0.8823\n",
            "Epoch 19/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3721 - accuracy: 0.8881 - val_loss: 0.3884 - val_accuracy: 0.8839\n",
            "Epoch 20/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.3632 - accuracy: 0.8905 - val_loss: 0.3818 - val_accuracy: 0.8861\n",
            "Epoch 21/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.3542 - accuracy: 0.8929 - val_loss: 0.3763 - val_accuracy: 0.8879\n",
            "Epoch 22/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.3452 - accuracy: 0.8956 - val_loss: 0.3677 - val_accuracy: 0.8913\n",
            "Epoch 23/100\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.3359 - accuracy: 0.8984 - val_loss: 0.3636 - val_accuracy: 0.8920\n",
            "Epoch 24/100\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.3267 - accuracy: 0.9012 - val_loss: 0.3583 - val_accuracy: 0.8943\n",
            "Epoch 25/100\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.3184 - accuracy: 0.9038 - val_loss: 0.3520 - val_accuracy: 0.8953\n",
            "Epoch 26/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.3091 - accuracy: 0.9069 - val_loss: 0.3480 - val_accuracy: 0.8976\n",
            "Epoch 27/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.3007 - accuracy: 0.9094 - val_loss: 0.3431 - val_accuracy: 0.8991\n",
            "Epoch 28/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2931 - accuracy: 0.9116 - val_loss: 0.3402 - val_accuracy: 0.8995\n",
            "Epoch 29/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2841 - accuracy: 0.9142 - val_loss: 0.3357 - val_accuracy: 0.9010\n",
            "Epoch 30/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2763 - accuracy: 0.9169 - val_loss: 0.3329 - val_accuracy: 0.9026\n",
            "Epoch 31/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2688 - accuracy: 0.9188 - val_loss: 0.3289 - val_accuracy: 0.9039\n",
            "Epoch 32/100\n",
            "100/100 [==============================] - 1s 11ms/step - loss: 0.2607 - accuracy: 0.9214 - val_loss: 0.3291 - val_accuracy: 0.9043\n",
            "Epoch 33/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2531 - accuracy: 0.9231 - val_loss: 0.3248 - val_accuracy: 0.9054\n",
            "Epoch 34/100\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.2454 - accuracy: 0.9260 - val_loss: 0.3251 - val_accuracy: 0.9055\n",
            "Epoch 35/100\n",
            "100/100 [==============================] - 1s 14ms/step - loss: 0.2388 - accuracy: 0.9277 - val_loss: 0.3244 - val_accuracy: 0.9056\n",
            "Epoch 36/100\n",
            "100/100 [==============================] - 1s 15ms/step - loss: 0.2310 - accuracy: 0.9302 - val_loss: 0.3202 - val_accuracy: 0.9075\n",
            "Epoch 37/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2240 - accuracy: 0.9325 - val_loss: 0.3186 - val_accuracy: 0.9073\n",
            "Epoch 38/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2176 - accuracy: 0.9341 - val_loss: 0.3178 - val_accuracy: 0.9089\n",
            "Epoch 39/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2104 - accuracy: 0.9365 - val_loss: 0.3169 - val_accuracy: 0.9093\n",
            "Epoch 40/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.2045 - accuracy: 0.9383 - val_loss: 0.3202 - val_accuracy: 0.9086\n",
            "Epoch 41/100\n",
            "100/100 [==============================] - 1s 12ms/step - loss: 0.1981 - accuracy: 0.9400 - val_loss: 0.3195 - val_accuracy: 0.9093\n",
            "Epoch 41: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferenceing and decoding on Bi LSTM Encoder and Lstm decoder\n",
        "\n"
      ],
      "metadata": {
        "id": "FpgRz_5HDprV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "model_1 = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/BiDirectional_lstm_Encoder_lstm_decoderV3.keras\")\n",
        "\n",
        "# Print layer names to identify required layers\n",
        "layer_names = [layer.name for layer in model_1.layers]\n",
        "print(layer_names)\n",
        "\n",
        "# Rebuild Encoder\n",
        "encoder_inputs = model_1.input[0]  # input_1\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = model_1.layers[1].output  # lstm_1 (Bidirectional LSTM)\n",
        "state_h_enc = keras.layers.Concatenate()([forward_h, backward_h])  # Concatenate forward and backward states\n",
        "state_c_enc = keras.layers.Concatenate()([forward_c, backward_c])  # Concatenate forward and backward states\n",
        "encoder_states = [state_h_enc, state_c_enc,encoder_outputs]\n",
        "encoder_model = Model(encoder_inputs,encoder_states)\n",
        "#Bi_GRU_encoder_model = Model(Bi_GRU_encoder_inputs, [Bi_GRU_encoder_outputs] + Bi_GRU_encoder_states)\n",
        "# Rebuild Decoder\n",
        "latent_dim = state_h_enc.shape[-1]  # Get the latent dimension size from the concatenated states\n",
        "decoder_inputs = model_1.input[1]  # input_2\n",
        "\n",
        "decoder_state_input_h = keras.Input(shape=(latent_dim ,))\n",
        "decoder_state_input_c = keras.Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_lstm = model_1.layers[5]\n",
        "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
        "    decoder_inputs, initial_state=decoder_states_inputs\n",
        ")\n",
        "decoder_states = [state_h_dec, state_c_dec]\n",
        "decoder_dense = model_1.layers[-1]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = keras.Model(\n",
        "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
        ")\n",
        "\n",
        "# Testing the rebuilt models (Optional)\n",
        "print(\"Encoder model summary:\")\n",
        "encoder_model.summary()\n",
        "print(\"Decoder model summary:\")\n",
        "decoder_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NLW_cYHuFxhj",
        "outputId": "c27f02f3-51a9-4920-8b60-cdeb570ea486"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['input_29', 'bidirectional_2', 'input_30', 'concatenate_18', 'concatenate_19', 'lstm_5', 'dense_3']\n",
            "Encoder model summary:\n",
            "Model: \"model_9\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_29 (InputLayer)       [(None, None, 70)]           0         []                            \n",
            "                                                                                                  \n",
            " bidirectional_2 (Bidirecti  [(None, 1024),               2387968   ['input_29[0][0]']            \n",
            " onal)                        (None, 512),                                                        \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512),                                                        \n",
            "                              (None, 512)]                                                        \n",
            "                                                                                                  \n",
            " concatenate_10 (Concatenat  (None, 1024)                 0         ['bidirectional_2[0][1]',     \n",
            " e)                                                                  'bidirectional_2[0][3]']     \n",
            "                                                                                                  \n",
            " concatenate_11 (Concatenat  (None, 1024)                 0         ['bidirectional_2[0][2]',     \n",
            " e)                                                                  'bidirectional_2[0][4]']     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2387968 (9.11 MB)\n",
            "Trainable params: 2387968 (9.11 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Decoder model summary:\n",
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_30 (InputLayer)       [(None, None, 91)]           0         []                            \n",
            "                                                                                                  \n",
            " input_11 (InputLayer)       [(None, 1024)]               0         []                            \n",
            "                                                                                                  \n",
            " input_12 (InputLayer)       [(None, 1024)]               0         []                            \n",
            "                                                                                                  \n",
            " lstm_5 (LSTM)               [(None, None, 1024),         4571136   ['input_30[0][0]',            \n",
            "                              (None, 1024),                          'input_11[0][0]',            \n",
            "                              (None, 1024)]                          'input_12[0][0]']            \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, None, 91)             93275     ['lstm_5[1][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4664411 (17.79 MB)\n",
            "Trainable params: 4664411 (17.79 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from collections import defaultdict\n",
        "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    #states_value=encoder_model.predict(input_seq, verbose=0)\n",
        "    encoder_output, state_h, state_c = encoder_model.predict(input_seq, verbose=0)\n",
        "\n",
        "    # Prepare initial states for the decoder.\n",
        "    states_value = [state_h, state_c]\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    stop_condition = False\n",
        "    decoded_sentence = \"\"\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length or find stop character.\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "\n",
        "    return decoded_sentence\n",
        "\n",
        "\n",
        "\n",
        "decoded_sentences=[]\n",
        "\n",
        "\n",
        "smooth = SmoothingFunction().method4\n",
        "bleu_score=0\n",
        "for seq_index in range(20):\n",
        "    input_seq = encoder_input_data_test[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    decoded_sentences.append(decoded_sentence)\n",
        "    print(f'Input text: {input_test[seq_index]}')\n",
        "    print(f'Target text: {target_test[seq_index]}')\n",
        "    print(f'Decoded text: {decoded_sentence}')\n",
        "    reference = target_test[seq_index].strip().split()  # Split target text into tokens\n",
        "    candidate = decoded_sentence.split()  # Split decoded text into tokens\n",
        "    bleu_score += sentence_bleu(reference, candidate, smoothing_function=smooth)  # Calculate BLEU score with equal weights for all n-grams\n",
        "\n",
        "\n",
        "\n",
        "print(\"Overall BLEU Score: \" + str(bleu_score/20))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ja4PZpkq2cHl",
        "outputId": "6734e121-1ad3-4d08-d080-1cd9cb9649c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text: Stop yelling!\n",
            "Target text: \tCessez de hurler !\n",
            "\n",
            "Decoded text: e portir !\n",
            "\n",
            "Input text: Anybody hurt?\n",
            "Target text: \tQuiconque a-t-il été blessé ?\n",
            "\n",
            "Decoded text: is  s                                                       \n",
            "Input text: All aboard!\n",
            "Target text: \tTout le monde à bord !\n",
            "\n",
            "Decoded text: ill ma gagnes !\n",
            "\n",
            "Input text: Are you lame?\n",
            "Target text: \tÊtes-vous boiteux ?\n",
            "\n",
            "Decoded text: i n  s                                                      \n",
            "Input text: Who are you?\n",
            "Target text: \tT'es qui toi ?\n",
            "\n",
            "Decoded text: is s ma gagné ?\n",
            "\n",
            "Input text: They came in.\n",
            "Target text: \tIls sont entrés.\n",
            "\n",
            "Decoded text: is s min chante !\n",
            "\n",
            "Input text: Get lost.\n",
            "Target text: \tDécampe !\n",
            "\n",
            "Decoded text: is le von  l                                                \n",
            "Input text: I don't date.\n",
            "Target text: \tJe ne sors pas avec des garçons.\n",
            "\n",
            "Decoded text: is s ma gagné !\n",
            "\n",
            "Input text: They love me.\n",
            "Target text: \tElles m'aiment.\n",
            "\n",
            "Decoded text: is pour !\n",
            "\n",
            "Input text: Sign up.\n",
            "Target text: \tInscris-toi.\n",
            "\n",
            "Decoded text: i n                                                         \n",
            "Input text: We're home.\n",
            "Target text: \tNous sommes chez nous.\n",
            "\n",
            "Decoded text: is le vies !\n",
            "\n",
            "Input text: Have some ham.\n",
            "Target text: \tPrenez du jambon.\n",
            "\n",
            "Decoded text: is la vies !\n",
            "\n",
            "Input text: I handled it.\n",
            "Target text: \tJe m'en suis occupé.\n",
            "\n",
            "Decoded text: is                                                          \n",
            "Input text: I'm not deaf.\n",
            "Target text: \tJe ne suis pas sourd.\n",
            "\n",
            "Decoded text: is                                                          \n",
            "Input text: She fainted.\n",
            "Target text: \tElle est tombée dans les pommes.\n",
            "\n",
            "Decoded text: is                                                          \n",
            "Input text: Let me sleep.\n",
            "Target text: \tLaissez-moi dormir.\n",
            "\n",
            "Decoded text: is tomber !\n",
            "\n",
            "Input text: Go ahead!\n",
            "Target text: \tVa !\n",
            "\n",
            "Decoded text: is pour !\n",
            "\n",
            "Input text: We'll take it.\n",
            "Target text: \tNous la prendrons.\n",
            "\n",
            "Decoded text: is la vieux.\n",
            "\n",
            "Input text: You're stuck.\n",
            "Target text: \tT'es planté.\n",
            "\n",
            "Decoded text: is s ma gagne !\n",
            "\n",
            "Input text: I walked here.\n",
            "Target text: \tJ'ai marché jusqu'ici.\n",
            "\n",
            "Decoded text: is                                                          \n",
            "Overall BLEU Score: 0.028264070336128194\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inferencing with Model 2 : Bi GRU Encoder and GRU decoder"
      ],
      "metadata": {
        "id": "QPL6OQN5PW9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "model2 = keras.models.load_model(\"/content/drive/MyDrive/Colab Notebooks/BiDirectional_gru_Encoder_gru_decoderV2.keras\")\n",
        "\n",
        "# Print layer names to identify required layers\n",
        "layer_names = [layer.name for layer in model2.layers]\n",
        "print(layer_names)\n",
        "\n",
        "# Rebuild Encoder\n",
        "Bi_GRU_encoder_inputs = model2.input[0]  # input_1\n",
        "Bi_GRU_encoder_outputs, forward_h, backward_h = model2.layers[1].output  #  (Bidirectional GRU)\n",
        "state_h_enc = keras.layers.Concatenate()([forward_h, backward_h])  # Concatenate forward and backward states\n",
        "\n",
        "Bi_GRU_encoder_states = [state_h_enc]\n",
        "Bi_GRU_encoder_model = Model(Bi_GRU_encoder_inputs, [Bi_GRU_encoder_outputs] + Bi_GRU_encoder_states)\n",
        "\n",
        "# Rebuild Decoder\n",
        "latent_dim = state_h_enc.shape[-1]  # Get the latent dimension size from the concatenated states\n",
        "GRU_decoder_inputs = model2.input[1]  # input_2\n",
        "GRU_decoder_state_input_h = keras.Input(shape=(latent_dim ,))\n",
        "GRU_decoder_states_inputs = [GRU_decoder_state_input_h]\n",
        "GRU_decoder = model2.layers[4]\n",
        "GRU_decoder_outputs, state_h_dec = GRU_decoder(\n",
        "    GRU_decoder_inputs, initial_state=GRU_decoder_states_inputs\n",
        ")\n",
        "GRU_decoder_states = [state_h_dec]\n",
        "GRU_decoder_dense = model2.layers[5]\n",
        "GRU_decoder_outputs = GRU_decoder_dense(GRU_decoder_outputs)\n",
        "GRU_decoder_model = keras.Model(\n",
        "    [GRU_decoder_inputs] + GRU_decoder_states_inputs, [GRU_decoder_outputs] + GRU_decoder_states\n",
        ")\n",
        "\n",
        "# Testing the rebuilt models (Optional)\n",
        "print(\"Encoder model summary:\")\n",
        "Bi_GRU_encoder_model.summary()\n",
        "print(\"Decoder model summary:\")\n",
        "GRU_decoder_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc09be9f-c625-4e6f-cd04-ab529913e5d0",
        "id": "OubVcaTTQ4L6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['input_3', 'bidirectional_1', 'input_4', 'concatenate_2', 'gru_1', 'dense_1']\n",
            "Encoder model summary:\n",
            "Model: \"model_13\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, None, 70)]           0         []                            \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirecti  [(None, 512),                503808    ['input_3[0][0]']             \n",
            " onal)                        (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " concatenate_13 (Concatenat  (None, 512)                  0         ['bidirectional_1[0][1]',     \n",
            " e)                                                                  'bidirectional_1[0][2]']     \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 503808 (1.92 MB)\n",
            "Trainable params: 503808 (1.92 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Decoder model summary:\n",
            "Model: \"model_14\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None, None, 91)]           0         []                            \n",
            "                                                                                                  \n",
            " input_14 (InputLayer)       [(None, 512)]                0         []                            \n",
            "                                                                                                  \n",
            " gru_1 (GRU)                 [(None, None, 512),          929280    ['input_4[0][0]',             \n",
            "                              (None, 512)]                           'input_14[0][0]']            \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, None, 91)             46683     ['gru_1[1][0]']               \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 975963 (3.72 MB)\n",
            "Trainable params: 975963 (3.72 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "def decode_sequence2(input_seq):\n",
        "    # Encode the input sequence to get initial states for decoder\n",
        "    encoder_output, state_h_enc = Bi_GRU_encoder_model.predict(input_seq, verbose=0)\n",
        "    states_value = [state_h_enc]\n",
        "    '''encoder_output, state_h_enc_forward, state_h_enc_backward = Bi_GRU_encoder_model.predict(input_seq, verbose=0)\n",
        "    states_value = [keras.layers.Concatenate()([state_h_enc_forward, state_h_enc_backward])]'''\n",
        "\n",
        "    # Initialize the target sequence with the start token\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Initialize an empty decoded sentence\n",
        "    decoded_sentence = \"\"\n",
        "    #decoded_sentences=[]\n",
        "\n",
        "\n",
        "    # Sampling loop for decoding\n",
        "    stop_condition = False\n",
        "    while not stop_condition:\n",
        "        # Predict the next token and update states\n",
        "        #output_tokens, h = GRU_decoder_model.predict([target_seq, states_value], verbose=0)\n",
        "\n",
        "        output_tokens, h = GRU_decoder_model.predict(\n",
        "          [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "\n",
        "\n",
        "        # Sample a token from the output distribution\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length or find stop character\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence for the next time step\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h]\n",
        "\n",
        "    return decoded_sentence\n",
        "decoded_sentences=[]\n",
        "smooth = SmoothingFunction().method4\n",
        "bleu_score=0\n",
        "for seq_index in range(20):\n",
        "    input_seq = encoder_input_data_test[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    decoded_sentences.append(decoded_sentence)\n",
        "    print(f'Input text: {input_test[seq_index]}')\n",
        "    print(f'Target text: {target_test[seq_index]}')\n",
        "    print(f'Decoded text: {decoded_sentence}')\n",
        "    reference = target_test[seq_index].strip().split()  # Split target text into tokens\n",
        "    candidate = decoded_sentence.split()  # Split decoded text into tokens\n",
        "    bleu_score += sentence_bleu(reference, candidate, smoothing_function=smooth)  # Calculate BLEU score\n",
        "\n",
        "print(\"Overall BLEU Score: \" + str(bleu_score/20))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8lQOjVIay26",
        "outputId": "a927f8a9-3030-4ca3-bc0b-f4100427d9cb"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text: Stop yelling!\n",
            "Target text: \tCessez de hurler !\n",
            "\n",
            "Decoded text: Arrêtez de vous dispuser !\n",
            "\n",
            "Input text: Anybody hurt?\n",
            "Target text: \tQuiconque a-t-il été blessé ?\n",
            "\n",
            "Decoded text: Y a-t-il que qu  suis ?\n",
            "\n",
            "Input text: All aboard!\n",
            "Target text: \tTout le monde à bord !\n",
            "\n",
            "Decoded text: Trapez calme-toi.\n",
            "\n",
            "Input text: Are you lame?\n",
            "Target text: \tÊtes-vous boiteux ?\n",
            "\n",
            "Decoded text: Est-ce que tu au ?\n",
            "\n",
            "Input text: Who are you?\n",
            "Target text: \tT'es qui toi ?\n",
            "\n",
            "Decoded text: Qui s'est échappé ?\n",
            "\n",
            "Input text: They came in.\n",
            "Target text: \tIls sont entrés.\n",
            "\n",
            "Decoded text: Elles ont vient.\n",
            "\n",
            "Input text: Get lost.\n",
            "Target text: \tDécampe !\n",
            "\n",
            "Decoded text: Dégage.\n",
            "\n",
            "Input text: I don't date.\n",
            "Target text: \tJe ne sors pas avec des garçons.\n",
            "\n",
            "Decoded text: Je ne sors pas pas.\n",
            "\n",
            "Input text: They love me.\n",
            "Target text: \tElles m'aiment.\n",
            "\n",
            "Decoded text: Ils m'aiment.\n",
            "\n",
            "Input text: Sign up.\n",
            "Target text: \tInscris-toi.\n",
            "\n",
            "Decoded text: Dégarrez-le.\n",
            "\n",
            "Input text: We're home.\n",
            "Target text: \tNous sommes chez nous.\n",
            "\n",
            "Decoded text: Nous sommes de la main.\n",
            "\n",
            "Input text: Have some ham.\n",
            "Target text: \tPrenez du jambon.\n",
            "\n",
            "Decoded text: Prenez entrer.\n",
            "\n",
            "Input text: I handled it.\n",
            "Target text: \tJe m'en suis occupé.\n",
            "\n",
            "Decoded text: Je t'ai sauvée.\n",
            "\n",
            "Input text: I'm not deaf.\n",
            "Target text: \tJe ne suis pas sourd.\n",
            "\n",
            "Decoded text: Je ne suis pas sourde.\n",
            "\n",
            "Input text: She fainted.\n",
            "Target text: \tElle est tombée dans les pommes.\n",
            "\n",
            "Decoded text: Elle as risté.\n",
            "\n",
            "Input text: Let me sleep.\n",
            "Target text: \tLaissez-moi dormir.\n",
            "\n",
            "Decoded text: Laisse-moi resier.\n",
            "\n",
            "Input text: Go ahead!\n",
            "Target text: \tVa !\n",
            "\n",
            "Decoded text: Allez-vous-en !\n",
            "\n",
            "Input text: We'll take it.\n",
            "Target text: \tNous la prendrons.\n",
            "\n",
            "Decoded text: Nous le prendrons.\n",
            "\n",
            "Input text: You're stuck.\n",
            "Target text: \tT'es planté.\n",
            "\n",
            "Decoded text: Vous êtes sarisse.\n",
            "\n",
            "Input text: I walked here.\n",
            "Target text: \tJ'ai marché jusqu'ici.\n",
            "\n",
            "Decoded text: J'ai vieux de moi.\n",
            "\n",
            "Overall BLEU Score: 0.011707284919851036\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference Model and Decoding Sequence for DOuble Layer GRU Encoder and GRU decoder"
      ],
      "metadata": {
        "id": "MrsrP63VhiBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Load the model\n",
        "model3 = load_model('/content/drive/MyDrive/Colab Notebooks/doubleLayer_gru_Encoder_gru_decoderV2.keras')\n",
        "for layer in model3.layers:\n",
        "    print(layer.name)\n",
        "encoder_temp=model3.input[0]\n",
        "out,h1=model3.layers[3].output\n",
        "e_states=[h1]\n",
        "encoder_model=Model(encoder_temp,e_states)\n",
        "\n",
        "#decoder_input = Input(shape=(None, 256))\n",
        "decoder_input=model3.input[1]\n",
        "#decoder_state_input_h=Input(shape=(512,))\n",
        "decoder_state_input_h=Input(shape=(latent_dim,))\n",
        "\n",
        "decoder_states_input =[decoder_state_input_h]\n",
        "decoder_gru=model3.layers[4]\n",
        "decoder_outputs, state_h_dec = decoder_gru(decoder_input, initial_state=decoder_states_input)\n",
        "decoder_dense = model3.layers[-1]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "decoder_model = Model(\n",
        "    [decoder_input] + [decoder_states_input],\n",
        "    [decoder_outputs, state_h_dec]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qd3zMTOYR_1",
        "outputId": "7f70a7a5-e742-4a35-d14c-2150249c9825"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_6\n",
            "gru_3\n",
            "input_7\n",
            "gru_4\n",
            "gru_5\n",
            "dense_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu,SmoothingFunction\n",
        "\n",
        "# Reverse-lookup token index to decode sequences back to\n",
        "# something readable.\n",
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())\n",
        "\n",
        "def decode_sequence(input_seq):\n",
        "    # Encode the input sequence to get initial states for decoder\n",
        "    state_h_enc =encoder_model.predict(input_seq, verbose=0)\n",
        "    states_value = [state_h_enc]\n",
        "\n",
        "\n",
        "    # Initialize the target sequence with the start token\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
        "\n",
        "    # Initialize an empty decoded sentence\n",
        "    decoded_sentence = \"\"\n",
        "    #decoded_sentences=[]\n",
        "\n",
        "\n",
        "    # Sampling loop for decoding\n",
        "    stop_condition = False\n",
        "    while not stop_condition:\n",
        "        # Predict the next token and update states\n",
        "        #output_tokens, h = GRU_decoder_model.predict([target_seq, states_value], verbose=0)\n",
        "\n",
        "        output_tokens, h = decoder_model.predict(\n",
        "          [target_seq] + states_value, verbose=0\n",
        "        )\n",
        "\n",
        "\n",
        "        # Sample a token from the output distribution\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char\n",
        "\n",
        "        # Exit condition: either hit max length or find stop character\n",
        "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence for the next time step\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.0\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h]\n",
        "\n",
        "    return decoded_sentence\n",
        "decoded_sentences=[]\n",
        "smooth = SmoothingFunction().method4\n",
        "bleu_score=0\n",
        "for seq_index in range(20):\n",
        "    input_seq = encoder_input_data_test[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    decoded_sentences.append(decoded_sentence)\n",
        "    print('Input text:',input_test[seq_index])\n",
        "    print('Target text:',target_test[seq_index].strip())\n",
        "    print('decoded text',decoded_sentence)\n",
        "    reference = target_test[seq_index].strip().split()  # Split target text into tokens\n",
        "    candidate = decoded_sentence.split()  # Split decoded text into tokens\n",
        "\n",
        "    bleu_score += sentence_bleu(reference, candidate, smoothing_function=smooth)  # Calculate BLEU score\n",
        "\n",
        "print(\"Overall BLEU Score: \" + str(bleu_score/20))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54GYC3Py-Xqs",
        "outputId": "44d69169-2ec1-4ed9-b2a1-fa5c40cb9f03"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input text: Stop yelling!\n",
            "Target text: Cessez de hurler !\n",
            "decoded text Arrêtez de viel !\n",
            "\n",
            "Input text: Anybody hurt?\n",
            "Target text: Quiconque a-t-il été blessé ?\n",
            "decoded text Y a-t-il que c'est ?\n",
            "\n",
            "Input text: All aboard!\n",
            "Target text: Tout le monde à bord !\n",
            "decoded text Bonne un !\n",
            "\n",
            "Input text: Are you lame?\n",
            "Target text: Êtes-vous boiteux ?\n",
            "decoded text Es-tu fou ?\n",
            "\n",
            "Input text: Who are you?\n",
            "Target text: T'es qui toi ?\n",
            "decoded text Qui est-il ?\n",
            "\n",
            "Input text: They came in.\n",
            "Target text: Ils sont entrés.\n",
            "decoded text Ils ont rieux.\n",
            "\n",
            "Input text: Get lost.\n",
            "Target text: Décampe !\n",
            "decoded text Décampez !\n",
            "\n",
            "Input text: I don't date.\n",
            "Target text: Je ne sors pas avec des garçons.\n",
            "decoded text Je ne peux pas vetir.\n",
            "\n",
            "Input text: They love me.\n",
            "Target text: Elles m'aiment.\n",
            "decoded text Ils a pas le main.\n",
            "\n",
            "Input text: Sign up.\n",
            "Target text: Inscris-toi.\n",
            "decoded text Pas un génie.\n",
            "\n",
            "Input text: We're home.\n",
            "Target text: Nous sommes chez nous.\n",
            "decoded text Nous sommes seuls.\n",
            "\n",
            "Input text: Have some ham.\n",
            "Target text: Prenez du jambon.\n",
            "decoded text Parle confinuer !\n",
            "\n",
            "Input text: I handled it.\n",
            "Target text: Je m'en suis occupé.\n",
            "decoded text Je n'ai pas échappé.\n",
            "\n",
            "Input text: I'm not deaf.\n",
            "Target text: Je ne suis pas sourd.\n",
            "decoded text Je ne suis pas une boulet.\n",
            "\n",
            "Input text: She fainted.\n",
            "Target text: Elle est tombée dans les pommes.\n",
            "decoded text Elle a eu lit.\n",
            "\n",
            "Input text: Let me sleep.\n",
            "Target text: Laissez-moi dormir.\n",
            "decoded text Laissez-moi forter.\n",
            "\n",
            "Input text: Go ahead!\n",
            "Target text: Va !\n",
            "decoded text Allez-vous !\n",
            "\n",
            "Input text: We'll take it.\n",
            "Target text: Nous la prendrons.\n",
            "decoded text Nous l'avons vu.\n",
            "\n",
            "Input text: You're stuck.\n",
            "Target text: T'es planté.\n",
            "decoded text Vous êtes sauves.\n",
            "\n",
            "Input text: I walked here.\n",
            "Target text: J'ai marché jusqu'ici.\n",
            "decoded text Je vous ai audui.\n",
            "\n",
            "Overall BLEU Score: 0.027883608196764505\n"
          ]
        }
      ]
    }
  ]
}